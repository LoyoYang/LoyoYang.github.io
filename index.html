
<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-52138338-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-52138338-2');
    </script>


    <meta name="generator" content="HTML Tidy"></meta>
    <link href="stylesheets/style.css" rel="stylesheet" type="text/css"></link>
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css"></link>
    <script src="js/hidebib.js" type="text/javascript"></script>
    <title>Luyu Yang</title>
    <description content="PhD Candidate in Computer Science"></description>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
    <td>
    <table width="100%" align="center" cellspacing="0" cellpadding="20">
      <p align="center">
        <font size="7">Luyu Yang</font>
        <br></br>
        <b>Email </b>
        <font id="email" style="display:inline;">loyo (at) umd (dot) edu</font>
      </p>
      <td width="65%" valign="middle" align="justify">
        <div id="includedContent"></div>
        <p>
          <p>
              I am a fourth-year PhD student at the
              <a href="https://www.umd.edu/">University of Maryland, College Park</a>. I am advised by
              <a href="http://www.cs.umd.edu/~abhinav/"> Prof. Abhinav Shrivastava  </a> and
              <a href="https://lsd.umiacs.io/"> Prof. Larry Davis </a>.
          </p>
          <p>
              My research currently is in machine learning and computer vision. I work on problems in domain adaptation and model robustness during my PhD. Before that, I have worked as an R&D for 3D reconstruction at <a href="https://www.kandaovr.com//">KanDao Technology</a>, a start-up company on VR cameras. During my masters, I have worked on action recognition in videos.
          </p>

          <p>
              Currently, I am seeking a full-time job. Please reach out to me if you are interested.
          </p>
        </p>

        <p align="center"> <a href="https://scholar.google.com/citations?user=n0nlmhoAAAAJ&hl=en">Google Scholar</a> | <a href="https://www.linkedin.com/in/luyu-yang-31a722a3/">LinkedIn</a> | <a href="docs/Luyu_Yang_cv.pdf">CV</a></p>
        <!-- <p align="center"> <a href="docs/Yogesh_Balaji_cv.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=0I2qH0oAAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/yogeshbalaji">Github</a> | <a href="https://twitter.com/yogeshbalaji95">Twitter</a> | <a href="https://www.linkedin.com/in/yogesh-balaji-0847b374/">LinkedIn</a></p> -->
      </td>
      <td width="35%" valign="top">
        <a href="images/luyu_img.jpg">
          <img src="images/luyu_img.jpg" width="90%" />
        </a>
      </td>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading id="news">News</sectionheading>
          <ul>
              <li>Two papers accepted by ECCV 2022.</li>
              <li>Nominated for Google PhD Fellowship 2022 by University of Maryland.</li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Research</sectionheading>
        </td>
      </tr>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tr>
        <td>
          <h2> <font color="gray">2022 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/pdf/2112.04345.pdf">
          <a href="https://arxiv.org/pdf/2112.04345.pdf">
            <img src="images/teaser_fig/Crodobo.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/pdf/2112.04345.pdf" id="2022_crodobo_paper"><paper_title>Burn After Reading: Online Adaptation for Cross-domain Streaming Data</paper_title></a>
          <br><strong>Luyu Yang</strong>, Mingfei Gao, Zeyuan Chen, Ran Xu, Abhinav Shrivastava, Chetan Ramaiah.<br><em>ECCV</em>, 2022<br>
          <div class="paper" id="2022_crodobo">
            <a href="https://arxiv.org/pdf/2112.04345.pdf">pdf</a>
            | <a href="javascript:toggleblock('2022_crodobo_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2022_crodobo')">bibtex</a>
            | <a href="https://github.com/salesforce/burn-after-reading">code</a>
            | <a href="projectpages/burnAfterReading/BurnAfterReading.html">Project page</a>
            <p align="justify"><i id="2022_crodobo_abs">
              In the context of online privacy, many methods propose complex security preserving measures to protect sensitive data. In this paper, we note that: not storing any sensitive data is the best form of security. We propose an online framework called "Burn After Reading", i.e. each online sample is permanently deleted after it is processed. Our framework utilizes the labels from the public data and predicts on the unlabeled sensitive private data. To tackle the inevitable distribution shift from the public data to the private data, we propose a novel domain adaptation algorithm that directly aims at the fundamental challenge of this online setting--the lack of diverse source-target data pairs. We design a Cross-Domain Bootstrapping approach, named CroDoBo, to increase the combined data diversity across domains. To fully exploit the valuable discrepancies among the diverse combinations, we employ the training strategy of multiple learners with co-supervision. CroDoBo achieves state-of-the-art online performance on four domain adaptation benchmarks.
            </i></p><pre xml:space="preserve">
            @article{yang2021burn,
            title={Burn After Reading: Online Adaptation for Cross-domain Streaming Data},
            author={Yang, Luyu and Gao, Mingfei and Chen, Zeyuan and Xu, Ran and Shrivastava, Abhinav and Ramaiah, Chetan},
            journal={arXiv preprint arXiv:2112.04345},
            year={2021}}
          </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2021 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Deep_Co-Training_With_Task_Decomposition_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf">
          <a href="hhttps://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Deep_Co-Training_With_Task_Decomposition_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf">
            <img src="images/teaser_fig/DeCoTa.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Deep_Co-Training_With_Task_Decomposition_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf">
          <paper_title>Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation</paper_title></a>
          <br><strong>Luyu Yang</strong>, Yan Wang, Mingfei Gao, Abhinav Shrivastava, Kilian Q. Weinberger, Wei-Lun Chao, Ser-Nam Lim.<br><em>ICCV</em>, 2021<br>
          <div class="paper" id="2021ICCV_decota">
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Deep_Co-Training_With_Task_Decomposition_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2021ICCV_decota_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2021ICCV_decota')">bibtex</a>
            | <a href="https://github.com/LoyoYang/DeCoTa">code</a>
            <p align="justify"><i id="2021ICCV_decota_abs">
              Semi-supervised domain adaptation (SSDA) aims to adapt models trained from a labeled source domain to a different but related target domain, from which unlabeled data and a small set of labeled data are provided. Current methods that treat source and target supervision without distinction overlook their inherent discrepancy, resulting in a source-dominated model that has not effectively use the target supervision. In this paper, we argue that the labeled target data needs to be distinguished for effective SSDA, and propose to explicitly decompose the SSDA task into two sub-tasks: a semi-supervised learning (SSL) task in the target domain and an unsupervised domain adaptation (UDA) task across domains. By doing so, the two sub-tasks can better leverage the corresponding supervision and thus yield very different classifiers. To integrate the strengths of the two classifiers, we apply the well established co-training framework, in which the two classifiers exchange their high confident predictions to iteratively 'teach each other' so that both classifiers can excel in the target domain. We call our approach Deep Co-training with Task decomposition (DeCoTa). DeCoTa requires no adversarial training and is easy to implement. Moreover, DeCoTa is well founded on the theoretical condition of when co-training would succeed. As a result, DeCoTa achieves state-of-the-art results on several SSDA datasets, outperforming the prior art by a notable 4% margin on DomainNet.
            </i></p><pre xml:space="preserve">
            @inproceedings{yang2021deep,
             title={Deep co-training with task decomposition for semi-supervised domain adaptation},
             author={Yang, Luyu and Wang, Yan and Gao, Mingfei and Shrivastava, Abhinav and Weinberger,
             Kilian Q and Chao, Wei-Lun and Lim, Ser-Nam},
             booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
             pages={8906--8916},
             year={2021}
            }
          </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2020 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590596.pdf">
          <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590596.pdf">
            <img src="images/teaser_fig/CMSS.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590596.pdf">
          <paper_title>Curriculum Manager for Source Selection in Multi-Source Domain Adaptation</paper_title></a>
          <br><strong>Luyu Yang</strong>, Yogesh Balaji, Ser-Nam Lim, Abhinav Shrivastava.<br><em>ECCV</em>, 2020<br>
          <div class="paper" id="2020EccvCMSS"><a href="https://arxiv.org/abs/2007.01261">pdf</a>
            | <a href="javascript:toggleblock('2020EccvCMSS_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020EccvCMSS')">bibtex</a>
              <br><p align="justify">
              <i id="2020EccvCMSS_abs">
                The performance of Multi-Source Unsupervised Domain Adaptation depends significantly on the effectiveness
                of transfer from labeled source domain samples. In this paper, we proposed an adversarial agent that
                learns a dynamic curriculum for source samples, called Curriculum Manager for Source Selection (CMSS).
                The Curriculum Manager, an independent network module, constantly updates the curriculum during training,
                and iteratively learns which domains or samples are best suited for aligning to the target. The intuition
                behind this is to force the Curriculum Manager to constantly re-measure the transferability of latent domains
                over time to adversarially raise the error rate of the domain discriminator. CMSS does not require any knowledge
                of the domain labels, yet it outperforms other methods on four well-known benchmarks by significant margins.
                We also provide interpretable results that shed light on the proposed method.
              </i></p><pre xml:space="preserve">
              @article{2020EccvCMSS,
              title={Curriculum Manager for Source Selection
                  in Multi-Source Domain Adaptation},
              author={Yang, Luyu and Balaji, Yogesh and
                  Lim, Ser-Nam and Shrivastava, Abhinav},
              journal={arXiv preprint arXiv:2007.01261},
              year={2020}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2018 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.pdf">
          <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.pdf">
            <img src="images/teaser_fig/PMGans.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.pdf">
          <paper_title>PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities</paper_title></a>
          <br>Lan Wang, Chenqiang Gao, <strong>Luyu Yang</strong>, Yue Zhao, Wangmeng Zuo, Deyu Meng.<br><em>ECCV</em>, 2018<br>
          <div class="paper" id="2018PMGans"><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2018PMGans_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2018PMGans')">bibtex</a>
              <br><p align="justify">
              <i id="2018PMGans_abs">
                Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new InfraredVisible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum.
              </i></p><pre xml:space="preserve">
              @inproceedings{wang2018pm,
              title={Pm-gans: Discriminative representation learning for action recognition using partial-modalities},
              author={Wang, Lan and Gao, Chenqiang and Yang, Luyu and Zhao, Yue and Zuo, Wangmeng and Meng, Deyu},
              booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
              pages={384--401},
              year={2018}}
            </pre></div></td>
      </tr>
      

    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Internships</sectionheading>
          <ul>
            <li> <a href="https://www.salesforceairesearch.com/">Salesforce Research </a> (Summer 2021) </li>
            <li> <a href="https://ai.facebook.com/">Facebook AI</a> (2020) </li>
            <li> <a href="https://en.megvii.com/">Megvii Research (US)</a> (Summer 2019) </li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Selected Awards</sectionheading>
          <ul>
            <li> Dean's Fellowship (2018-2019)</li>
          </ul>
        </td>
      </tr>
    </table>
  </body>
  <script xml:space="preserve" language="JavaScript">hideallbibs();</script>
  <!-- <script xml:space="preserve" language="javascript">hideblock('2022_crodobo');</script> -->
  <!-- <script xml:space="preserve" language="javascript">hideblock('2021ICCV_decota');</script> -->
  <!-- <script xml:space="preserve" language="javascript">hideblock('2020EccvCMSS_abs');</script> -->
  <!-- <script xml:space="preserve" language="javascript">hideblock('2018PMGans');</script> -->

</html>
